{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "pythonjvsc74a57bd0d2e8a3a76cde361bcff9830bb93c3c0c3c1a771270de526adc76c0bd6befe53e",
   "display_name": "Python 3.8.9 64-bit ('venv')"
  },
  "metadata": {
   "interpreter": {
    "hash": "d2e8a3a76cde361bcff9830bb93c3c0c3c1a771270de526adc76c0bd6befe53e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install sklearn jupyter pandas category-encoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FOLDER_NAME = \"titanic\"\n",
    "\n",
    "validatin_X = pd.read_csv(os.path.join(FOLDER_NAME, \"test.csv\"))\n",
    "train = pd.read_csv(os.path.join(FOLDER_NAME, \"train.csv\"))\n",
    "\n",
    "# Drop useless cols\n",
    "drop_cols = [\"Name\", \"PassengerId\"]\n",
    "test_X = test_X.drop(columns=drop_cols)\n",
    "train = train.drop(columns=drop_cols)\n",
    "\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Train dataset has {train.shape[0]} rows and {train.shape[1]} columns.')\n",
    "print(f'Test dataset has {test_X.shape[0]} rows and {test_X.shape[1]} columns.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train.isna().sum() / train.shape[0])\n",
    "print(\"-\" * 25)\n",
    "print(test_X.isna().sum() / test_X.shape[0])"
   ]
  },
  {
   "source": [
    "Most columns are full and both test and train datasets have same distribution of NaN values."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets explore the data\n",
    "sns.set_theme(style=\"darkgrid\")\n",
    "sns.displot(train, x=\"Age\", col=\"Survived\", row=\"Sex\")"
   ]
  },
  {
   "source": [
    "As you can see females were more likely to survive rather than males from titanic"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot(train, x=\"Fare\", col=\"Survived\", row=\"Pclass\", binwidth=10)"
   ]
  },
  {
   "source": [
    "Looks like the kids didnt survive the titanic either"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Dataset is not horribly imbalanced"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_Y = train[\"Survived\"]\n",
    "train_X = train.drop(columns=\"Survived\")\n",
    "del train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_Y.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets binary encode the sex values\n",
    "\n",
    "label_encoder = LabelEncoder().fit(train_X[\"Sex\"])\n",
    "train_X[\"SexEnc\"] = label_encoder.transform(train_X[\"Sex\"])\n",
    "train_X = train_X.drop(columns=\"Sex\")\n",
    "test_X[\"SexEnc\"] = label_encoder.transform(test_X[\"Sex\"])\n",
    "test_X = test_X.drop(columns=\"Sex\")\n",
    "\n",
    "del label_encoder\n",
    "train_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"There are {train_X['Embarked'].isna().sum()} NaN values in train_X['Embarked']\")\n"
   ]
  },
  {
   "source": [
    "Let us convert NaN to most frequent value for \"Embarked\" since it is 2/890 which is insignificant. Then we can one-hot encode the \"Embarked\" value."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def onehot_encode_embarked(train_X, test_X):\n",
    "    train_embarked = train_X[\"Embarked\"].to_numpy().reshape(-1, 1)\n",
    "    test_embarked = test_X[\"Embarked\"].to_numpy().reshape(-1, 1)\n",
    "\n",
    "    imputer = SimpleImputer(missing_values=np.nan, strategy='most_frequent')\n",
    "    train_embarked = imputer.fit_transform(train_embarked)\n",
    "    test_embarked = imputer.transform(test_embarked)\n",
    "\n",
    "    onehot_encoder = OneHotEncoder(sparse=False).fit(train_embarked)\n",
    "    train_embarked_enc = onehot_encoder.transform(train_embarked)\n",
    "    test_embarked_enc = onehot_encoder.transform(test_embarked)\n",
    "\n",
    "    # Sanity check to see that data is encoded the same way in test and train\n",
    "    print(test_embarked_enc[0], test_embarked[0])\n",
    "    print(train_embarked_enc[0], train_embarked[0])\n",
    "    categories = [f\"Embarked_{c}\" for c in onehot_encoder.categories_[0]]\n",
    "\n",
    "    train_emb_df = pd.DataFrame(data=train_embarked_enc, columns=categories)\n",
    "    test_emb_df = pd.DataFrame(data=test_embarked_enc, columns=categories)\n",
    "\n",
    "    train_X = pd.concat([train_X, train_emb_df], axis=1).drop(columns=\"Embarked\")\n",
    "    test_X = pd.concat([test_X, test_emb_df], axis=1).drop(columns=\"Embarked\")\n",
    "    return train_X, test_X\n",
    "\n",
    "train_X, test_X = onehot_encode_embarked(train_X, test_X)\n",
    "\n",
    "# Lets drop Embarked_S as that value is already encoded implicity. It adds no new information\n",
    "train_X = train_X.drop(columns=\"Embarked_S\")\n",
    "test_X = test_X.drop(columns=\"Embarked_S\")\n",
    "\n",
    "train_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"There are {train_X['Age'].isna().sum()} NaN values in train_X[Age]\")\n",
    "print(f\"There are {train_X['Fare'].isna().sum()} NaN values in train_X[Fare]\")\n",
    "print(f\"The mean for SexEnc == 1 is {train_X[train_X['SexEnc'] == 1]['Age'].mean()}\")\n",
    "print(f\"The mean for SexEnc == 0 is {train_X[train_X['SexEnc'] == 0]['Age'].mean()}\")"
   ]
  },
  {
   "source": [
    "Let us replace the missing ages with different means for males and females"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_1 = train_X['SexEnc'] == 1 & train_X['Age'].isna()\n",
    "train_X[\"Age\"] = train_X[\"Age\"].mask(cond=mask_1, \n",
    "                                     other=train_X[train_X['SexEnc'] == 1]['Age'].mean())\n",
    "\n",
    "mask_2 = train_X['SexEnc'] == 0 & train_X['Age'].isna()\n",
    "train_X[\"Age\"] = train_X[\"Age\"].mask(cond=mask_2, \n",
    "                                     other=train_X[train_X['SexEnc'] == 0]['Age'].mean())\n",
    "\n",
    "print(f\"There are {train_X['Age'].isna().sum()} NaN values in train_X[Age]\")\n",
    "train_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us do the same calc for test set\n",
    "\n",
    "mask_1 = test_X['SexEnc'] == 1 & test_X['Age'].isna()\n",
    "test_X[\"Age\"] = test_X[\"Age\"].mask(cond=mask_1, \n",
    "                                     other=test_X[test_X['SexEnc'] == 1]['Age'].mean())\n",
    "\n",
    "mask_2 = test_X['SexEnc'] == 0 & test_X['Age'].isna()\n",
    "test_X[\"Age\"] = test_X[\"Age\"].mask(cond=mask_2, \n",
    "                                     other=test_X[test_X['SexEnc'] == 0]['Age'].mean())\n",
    "\n",
    "print(f\"There are {test_X['Age'].isna().sum()} NaN values in test_X[Age]\")\n",
    "test_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"There are {train_X['Ticket'].nunique()} unique values in Ticket column in train\")\n",
    "print(f\"There are {test_X['Ticket'].nunique()} unique values in Ticket column in test\")"
   ]
  },
  {
   "source": [
    "One-hot/base-k style encoding it would make the number of features very high. This would impact our ML performance. This would be the perfect candidate for Target Encoding. \n",
    "\n",
    "Read this [blog](https://maxhalford.github.io/blog/target-encoding/) to get a better understanding."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smooth_mean(train_X, train_Y, test_X, m):\n",
    "    temp = pd.concat([train_X, train_Y], axis=1)\n",
    "\n",
    "    # Prior survival prob\n",
    "    prior = temp[\"Survived\"].mean()\n",
    "\n",
    "    # Compute the number of values and the mean of each group\n",
    "    train_agg = temp.groupby(\"Ticket\")[\"Survived\"].agg(['count', 'mean'])\n",
    "    counts = train_agg['count']\n",
    "    means = train_agg['mean']\n",
    "\n",
    "    # Compute the \"smoothed\" means for train dataset\n",
    "    smooth = (counts * means + m * prior) / (counts + m)\n",
    "\n",
    "    # Replace each value by the according smoothed mean in train\n",
    "    # and test\n",
    "    train_X[\"Ticket\"] = temp[\"Ticket\"].map(smooth)\n",
    "    test_X[\"Ticket\"] = test_X[\"Ticket\"].map(smooth).fillna(prior)\n",
    "\n",
    "    return train_X, test_X\n",
    "\n",
    "train_X, test_X = smooth_mean(train_X, train_Y, test_X, 200)\n",
    "train_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's drop the Cabin column since more than 70% of it is NaN anyway\n",
    "train_X = train_X.drop(columns=\"Cabin\")\n",
    "test_X = test_X.drop(columns=\"Cabin\")\n",
    "\n",
    "# Lets also impute missing fares\n",
    "def impute_fares(train_X, test_X):\n",
    "    train_fare = train_X[\"Fare\"].to_numpy().reshape(-1, 1)\n",
    "    test_fare = test_X[\"Fare\"].to_numpy().reshape(-1, 1)\n",
    "\n",
    "    imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "    imputer.fit(train_fare)\n",
    "    train_fare = imputer.transform(train_fare)\n",
    "    test_fare = imputer.transform(test_fare)\n",
    "\n",
    "    train_fare_df = pd.DataFrame(data=train_fare, columns=[\"Fare\"])\n",
    "    test_fare_df = pd.DataFrame(data=test_fare, columns=[\"Fare\"])\n",
    "\n",
    "    train_X[\"Fare\"] = train_fare_df[\"Fare\"]\n",
    "    test_X[\"Fare\"] = test_fare_df[\"Fare\"]\n",
    "\n",
    "    return train_X, test_X\n",
    "    \n",
    "train_X, test_X = impute_fares(train_X, test_X)\n",
    "\n",
    "print(train_X.isna().sum())\n",
    "print(test_X.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "clf = DecisionTreeClassifier(random_state=0)\n",
    "clf.fit(train_X, train_Y)\n",
    "\n",
    "Y_pred = clf.predict(test_X)\n",
    "print(confusion_matrix(train_Y, Y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}